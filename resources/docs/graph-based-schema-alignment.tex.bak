\documentclass[12pt,twoside,a4paper]{article}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\begin{document}
\title{Graph based schema alignment for high volume file conversion}
\author{R.H. van 't Veer, MA}
\maketitle
\begin{abstract}
When confronted with a deluge of files to be converted to Linked Data, where are we to begin? Where most solutions focus on converting a single table or file, real-world cases often encompass a multitude of files. Few solutions exist for dealing with where full conversion is infeasible, where thousands of files, scattered over hundreds of schemas. The solution provided here explores the idea of graph-based schema comparison for scaling data conversion of thousands of files. It concludes that this approach is helpful in charting similarities and differences, provides grip and insight on a starting point, on progress and offers estimations on the required effort on transforming all data.
{\bf Keywords:} Linked Data, conversion, triplification, scaling, schema alignment
\end{abstract}
\section{Introduction}
\paragraph{•}
When undertaking the endeavour to convert legacy tabular data into Linked Data - for whatever reason - the main time investment most probably lies in the application of the proper vocabulary, creating an ontology and describing all classes and properties. This is because usually it is easy to create an automated or 'direct'\cite{michel:hal-00903568} mapping from a data table that produces linked data using a dummy prefix. The general approach is to append to a provided prefix: the table names for class names, the column names for properties, the primary key as identifier for instance data. The resulting output does support graph analysis, but is lacking in semantics, so there is little gained in publishing the direct mapping as is. The data may technically be RDF, but it will be four stars instead of five, as the famous canon\cite{5-star-open-data} of Linked Data goes. The brunt of manual labour involved in triplifying legacy data is in restructuring the data (if needed) and creating new classes and properties or choosing existing ones from shared vocabularies. So, to create data that is machine-readable, we must undertake the effort of explaining the structure of our data.
\paragraph{•}
When we accept the assumption that the data modelling step is the most time-consuming part of triplification, it must follow that if we are confronted with a situation where the conversion of data must scale, the standard direct mapping solution will fail due to the time invested in creating the semantic model. If conversion of thousands, or hundreds of thousands of files or tables is called for, it becomes evident that a file-by-file conversion system that requires manual input for every single file will take too much time. If, say, a collection of 100,000 files requires one hour for every file to be processed, 2,500 full working weeks or around 50 years are required for the process to finish, depending on how many holidays are granted by your contract. Even if no more than one minute of manual labour is involved, the better part of a full year of undivided attention to manual input is required. We must aim much higher. Ideally, we wouldn't want to invest more than a few weeks - so the goal is to reduce the per-source manual mapping effort to no more than a few seconds, perhaps as few as five.  
\paragraph{•}
In order to prevent making a career out of converting a large volume of sources, I will propose a method for automating the steps that precede the actual mapping. It revolves around re-using as much information from the modelling stage as possible. Yet, there are surprisingly few solutions that provide such far-reaching automation of Linked Data mapping. The only solution that I found re-using previously applied mappings is a package called Karma.\footnote{https://github.com/usc-isi-i2/Web-Karma} Karma offers handy mapping suggestions based on settings applied earlier\footnote{https://youtu.be/h3\_yiBhAJIc?t=155}, but still requires file-by-file mapping. If we are to reach our five-second goal, we need automation that does bulk transformation on a large volume of files simultaneously.
\section{Case study: the Dutch repository of archaeological data files}
If many thousands of sources are involved, chances are that a large number of source schemas are present as well, but considerably less than the number of sources to begin with. This schema variability is something we can use to our advantage. With our test case, this is the case as well. The case in point is the national repository for archaeological data. The Netherlands can boast an extensive collection of digital archaeological data, gathered by various institutions and submitted to the Data Archiving and Networked Services (DANS) organisation, itself a sub-organisation of the Royal Netherlands Academy of Arts and Sciences. At the moment of writing, the DANS repository holds close to twenty-seven thousand deposited data sets on the topic of archaeology. The concept of a centralized data repository for archaeological data is unfortunately still much of a rarity, let alone instances that hold this much information.
\paragraph{•}
However, there is the central problem to the accessibility of the data in this national repository. The Netherlands may take pride in an archaeological data repository of repute, but its access leaves much to be desired. No data contained in the repository, other than on project-level metadata, can be searched for information of interest. Thus, it does not allow searching for specific artifact or archaeological structure types, nor does it support geospatial search beyond the centroid locations on the project level. Only results from text searches on comments supplied in the project description can be provided.
\paragraph{•}
What is needed, is a solution that provides full semantic search capabilities on artifacts and other archaeological phenomena, that allows researchers to do deep and complete queries for very specific problems. This is to be the main aim of this research outline: to investigate a solution that opens up the full search access to the Dutch corpus of archaeological data.
\paragraph{•}
The DANS archaeological repository is basically a collection of semi-structured data, commonly collected in the field and office through relatively simple databases and spreadsheets, converted into flattened tabular data. Although the archaeological sector has agreed on a common data exchange format (\textit{SIKB0102} or more popularly named the \textit{Pakbon} protocol) for the Netherlands, its uptake is still very much limited and its normative model does not lend itself very well to the faults and omissions that occur in the unstructured data. Historically, data has been gathered for a long time through databases and spreadsheets that do not conform to the strict referential integrity that the SIKB0102 protocol enforces, so there is a big backwards compatibility problem. The bulk of the current archaeological data simply cannot be expressed through this protocol, since it would violate most of its constraints.
\paragraph{•}
This is hardly surprising when we take into account the variation of schemas that define the tabular data. It varies not only from organisation to organisation, but also in respect to project types - Medieval town excavations with complex three-dimensional stratigraphy require a different approach than digs on Mesolithic sites that often focus on collecting flint artefacts in sampling units that conform to standardized sizes. It is also not uncommon to adapt the database schema to the specific needs of the project, resulting in single-instance schema variations. So, the DANS archaeological repository seems the ideal candidate for exploring a method to convert many thousands of files, spread over several hundreds of schemas. 
\bibliographystyle{plain}
\bibliography{graph-based-schema-alignment}
\end{document}
